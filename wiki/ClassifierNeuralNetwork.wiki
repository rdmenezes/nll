#summary Example of a Neural network classifier

= Neural network classifier =
== What is a neural network ==
Artificial neural networks are computers whose architecture is modeled after the brain. They typically consist of many hundreds of simple processing units which are wired together in a complex communication network. Each unit or node is a simplified model of a real neuron which fires (sends off a new signal) if it receives a sufficiently strong input signal from the other nodes to which it is connected. The strength of these connections may be varied in order for the network to perform different tasks corresponding to different patterns of node firing activity. This structure is very different from traditional computers.

== Multi-layered Perceptron architecture ==
MLP is a Feed-forward neural networks, where the data is propagated from input to output units in a strictly feedforward manner. The data processing can extend over multiple layers of units, but no feedback connections are present.

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/net.png

The bias nodes are nodes that are always activated.

=== Boundary decision ===
http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/bondary.png

Example of boundary decisions for a two class two attributs problem. The red decision boundary on the right has clearly overfitted.

=== Propagation ===
For each output unit of each layer, we define:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq1.gif

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq2.gif

With http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq3.gif

=== Training via Back-propagation ===
We define mean squared error as

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq4.gif

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq5.gif

with _p_ one of the pattern, _d_ the expected pattern and _y_ the output of the network.

We define the backpropagation error as:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq6.gif

The weights are updated as below:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq7.gif

=== Code ===
It is assumed the training pattern are centered on zero and have one variance for each attribut.

{{{
void test()
{
   srand( 1 );

   // find the correct benchmark
   const nll::benchmark::BenchmarkDatabases::Benchmark* benchmark = nll::benchmark::BenchmarkDatabases::instance().find( "cancer1.dt" );
   ensure( benchmark, "can't find benchmark" );
   Classifier::Database dat = benchmark->database;

   // use a multi layered perceptron as a classifier
   typedef algorithm::ClassifierMlp<Input> ClassifierImpl;
   ClassifierImpl classifier;

   // optimize the parameters of the classifier on the original dataset
   // we will use an harmony search algorithm.
   // for each point, the classifier is evaluated: a 10-fold cross validation is
   // runned on the learning database
   Classifier::OptimizerClientClassifier classifierOptimizer = classifier.createOptimizer( dat );

   nll::algorithm::StopConditionIteration stop( 10 );
   nll::algorithm::MetricEuclidian<nll::algorithm::OptimizerHarmonySearchMemory::TMetric::value_type> metric;

   nll::algorithm::OptimizerHarmonySearchMemory parametersOptimizer( 5, 0.8, 0.1, 1, &stop, 0.01, &metric );
   std::vector<double> params = parametersOptimizer.optimize( classifierOptimizer, ClassifierImpl::buildParameters() );
   
   // learn the LEARNING and VALIDATION database with the optimized parameters, and test the classifier
   // on the TESTING database
   classifier.learnTrainingDatabase( dat, nll::core::make_buffer1D( params ) );
   Classifier::Result rr = classifier.test( dat );

   TESTER_ASSERT( rr.testingError < 0.025 );
}
}}}