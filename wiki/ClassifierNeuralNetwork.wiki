#summary Example of a Neural network classifier

= Neural network classifier =
== What is a neural network ==
Neural Network is a powerful tool used in modern intelligent systems. Many applications that involve pattern recognition, feature mapping, clustering, classification and etc. use Neural Networks as an essential component. Many types of neural networks have been developed. Back Error Propagation, Kohonen feature map and Hopfield network are some of basic networks that have been developed and are used in many applications.

In this article general Multi Layer Perceptron and Back Error Propagation is presented. The sample application uses this Multi Layer Perceptron and classifies the blue and red patterns generated by user.

== Multi-layered Perceptron architecture ==
MLP is a Feed-forward neural networks, where the data is propagated from input to output units in a strictly feedforward manner. The data processing can extend over multiple layers of units, but no feedback connections are present.

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/net.png

The number of layers and the number of processing elements per layer are important decisions. There is no quantifiable, best answer to the layout of the network for any particular application. There are only general rules picked up over time and followed by most researchers and engineers applying this architecture to their problems.
A one-hidden layer MLP is an universal approximator, i.e., it can approximate any function to any desired accuracy. However the number of hidden nodes to achieve is accuracy is growing exponentially.

=== Propagation ===
For a particular layer and for each node of this layer, we define:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq1.gif

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq2.gif

With http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq3.gif

Note that y0 represents the bias of the network and is commonly modeled by adding and extra node which is always activated from the first layer to the n-1 layer.

It is then propagated to the successive layers until the output layer. The function f is called the activation function and this is what makes the network nonlinear. For backpropagation learning, the activation function must be differentiable, and it helps if the function is
bounded. a common used activation functions is the sigmoid defined as:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq8.gif

And its graph:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/fig1.png

It can be seens that the function is saturating quickly outside the interval [ -1..1 ], and as a consequence the input patterns must be normalized so that the mean of each feature is 0 and its variance is 1.

=== Training via Back-propagation ===
The feedforward, back-propagation architecture was developed in the early 1970's by several independent sources (Werbor; Parker; Rumelhart, Hinton and Williams). This independent co-development was the result of a proliferation of articles and talks at various conferences which stimulated the entire industry. Currently, this synergistically developed back-propagation architecture is the most popular, effective, and easy-to-learn model for complex, multi-layered networks.  Its greatest strength is in non-linear solutions to ill-defined problems. The typical back-propagation network has an input layer, an output layer, and at least one hidden layer. There is no theoretical limit on the number of hidden layers but typically there are just one or two.

We define mean squared error as

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq4.gif

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq5.gif

with _p_ one of the pattern, _d_ the expected output pattern and _y_ the output of the network.

We define the backpropagation error as:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq6.gif

The weights are updated as below:

http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/eq7.gif

Now, lets work on computing this weight update:

=== Boundary decision ===
http://nll.googlecode.com/svn/devl/wiki/NeuralNetworks/bondary.png

Example of boundary decisions for a two class two attributs problem. The red decision boundary on the right has clearly overfitted.


=== Code ===
It is assumed the training pattern are centered on zero and have one variance for each attribut.

{{{
void test()
{
   srand( 1 );

   // find the correct benchmark
   const nll::benchmark::BenchmarkDatabases::Benchmark* benchmark = nll::benchmark::BenchmarkDatabases::instance().find( "cancer1.dt" );
   ensure( benchmark, "can't find benchmark" );
   Classifier::Database dat = benchmark->database;

   // use a multi layered perceptron as a classifier
   typedef algorithm::ClassifierMlp<Input> ClassifierImpl;
   ClassifierImpl classifier;

   // optimize the parameters of the classifier on the original dataset
   // we will use an harmony search algorithm.
   // for each point, the classifier is evaluated: a 10-fold cross validation is
   // runned on the learning database
   Classifier::OptimizerClientClassifier classifierOptimizer = classifier.createOptimizer( dat );

   nll::algorithm::StopConditionIteration stop( 10 );
   nll::algorithm::MetricEuclidian<nll::algorithm::OptimizerHarmonySearchMemory::TMetric::value_type> metric;

   nll::algorithm::OptimizerHarmonySearchMemory parametersOptimizer( 5, 0.8, 0.1, 1, &stop, 0.01, &metric );
   std::vector<double> params = parametersOptimizer.optimize( classifierOptimizer, ClassifierImpl::buildParameters() );
   
   // learn the LEARNING and VALIDATION database with the optimized parameters, and test the classifier
   // on the TESTING database
   classifier.learnTrainingDatabase( dat, nll::core::make_buffer1D( params ) );
   Classifier::Result rr = classifier.test( dat );

   TESTER_ASSERT( rr.testingError < 0.025 );
}
}}}