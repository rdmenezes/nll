= Numerical learning library =

NLL is a multi-platform open source project entirely written in C++. Its goal it to propose generic and efficient algorithms for machine learning and more specifically computer vision. It is intended to be very easy to integrate and it is mainly composed of header files with no dependency on any library but the STL.

= Architecture =

NLL implements generic algorithms using template metaprogramming and a minimalist interface. Several layers are used:
  * core: the very basic structures and operations
  * algorithm_impl: generic algorithms with very limited dependencies and interface
  * algorithm: algorithms taking advantage of the NLL framework
  * preprocessing: it is built around the concept of typelist and propose a unified pipeline to process and optimize features. The processing of the data must comply with a specific set of transformations to eventually feed a classifier's inputs.

= Details =

Here is an overview of some algorithms implemented in NLL:
  * classifiers (k-nearest neighbour, multi-layered neural networks, support vector machines, boosting, gaussian mixture model)
  * feature selection (best-first, wrapper using genetic algorithm, relief-f, pearson)
  * feature transformation (PCA)
  * optimizers (grid search, harmony search, genetic algorithms)
  * math library (matrix, vector, linear algebra, distributions)
  * image library (resampling, morphology, transformations, convolutions, region growing, labelling)
  * clustering (k-means, LSDBC)
  * kd-trees, gabor filters... and much more soon!

= Example =

Here is a typical use of the framework:
{{{
/**
 In this test a neural network will be optimized using a harmony search algorithm.
 */
void test()
{
   typedef nll::benchmark::BenchmarkDatabases::Database::Sample::Input  Input;
   typedef nll::algorithm::Classifier<Input>                            Classifier;

   // find the cancer1.dt benchmark
   const nll::benchmark::BenchmarkDatabases::Benchmark* benchmark = nll::benchmark::BenchmarkDatabases::instance().find( "cancer1.dt" );
   ensure( benchmark, "can't find benchmark" );
   Classifier::Database dat = benchmark->database;

   // use a multi layered perceptron as a classifier
   typedef algorithm::ClassifierMlp<Input> ClassifierImpl;
   ClassifierImpl classifier;

   // optimize the parameters of the classifier on the original dataset
   // we will use a harmony search algorithm.
   // For each point, the classifier is evaluated: a 10-fold cross validation is
   // run on the learning database
   Classifier::OptimizerClientClassifier classifierOptimizer = classifier.createOptimizer( dat );

   // configure the optimizer options
   nll::algorithm::StopConditionIteration stop( 10 );
   nll::algorithm::MetricEuclidian<nll::algorithm::OptimizerHarmonySearchMemory::TMetric::value_type> metric;
   nll::algorithm::OptimizerHarmonySearchMemory parametersOptimizer( 5, 0.8, 0.1, 1, &stop, 0.01, &metric );

   // run the optimizer on the default constrained classifier parameters
   // if the default values don't fit, other constraint parameters should be given
   std::vector<double> params = parametersOptimizer.optimize( classifierOptimizer, ClassifierImpl::buildParameters() );
   
   // learn the TEST and VALIDATION database with the optimized parameters, and test the classifier
   // on the TESTING database
   classifier.learnTrainingDatabase( dat, nll::core::make_buffer1D( params ) );
   Classifier::Result rr = classifier.test( dat );

   TESTER_ASSERT( rr.testingError < 0.025 );
}
}}}