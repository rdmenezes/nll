\documentclass[11pt]{article}
\usepackage[utf8]{inputenc} % set input encoding to utf8
\usepackage[hmargin=3.5cm,vmargin=2.5cm]{geometry}
\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{bm} % bold math

\newcommand{\nllversion}{0.17}
\newcommand{\bigO}{\mathcal{O}}

\newcommand{\nllref}[1]{[\small{\textit{#1}}]}

%% configuration
%\usepackage{geometry}
%\setlength{\headheight}{0.6in}
%\setlength{\headsep}{25pt}
%\setlength{\topmargin}{0pt}
%\setlength{\footskip}{25pt}
%\setlength{\marginparsep}{11pt}

\title{Numerical learning library \nllversion~algorithm documentation}
\author{Ludovic Sibille}

\begin{document}

\maketitle
\newpage
\tableofcontents % the asterisk means that the contents itself isn't put into the ToC
\newpage

\pagestyle{fancy}
\lhead{Numerical Learning Library \nllversion}
\rhead{nll.googlecode.com}

\section{Introduction}
This document will describe the main algorithms used in the Numerical Learning Library \nllversion, how they are implemented and the assumptions made. It is meant to be as self-contained as possible, although some technical points requiring a great length of details will be referenced.

\section{Other Algorithms}
\subsection{Hessian approximation FD \nllref{HessianCalculatorForwardFiniteDifference}}
Using finite differences, we have:
\begin{align}
\frac{\partial f(x)}{\partial x_i \partial x_j} &= \frac{\partial}{\partial x_j}( \frac{\partial f(x)}{x_i}) \nonumber\\
&= \frac{\partial}{\partial x_j}( \frac{f(x + he_i) - f(x)}{h} ) \nonumber\\
&= \frac{1}{h}( \frac{f(x + he_i + he_j) - f(x + he_i)}{h} - \frac{f(x + he_j) - f(x)}{h} ) \nonumber\\
&= \frac{f(x + he_i + he_j) - f(x + he_i)-f(x + he_j)+f(x)}{h^2}
\end{align}

with $h$ close to 0 and $e_k$ the basis vector for the parameter $k$. See~\cite{Scolnik01} for other implementations and drawbacks. Note that the step size $h$ must be sufficiently low for a good accuracy but it must be as well sufficiently high to avoid underflow problems; This is problem dependent.

\section{Optimization algorithms}
\subsection{Newton's method \nllref{OptimizerNewton}}
The Newton's method finds a \textbf{local extremum} (i.e., minimum or maximum) by constructing a sequence $x_n$ from an initial guess $x_0$ that converges towards $x_*$ such that the function $\nabla_f(x_n)=0$. Consequently finding a global extremum is not guaranteed. The closer the initial guess is from the global extremum, the more likely it is to find the global extremum.

It is assuming the function $f$ is twice differentiable and locally approximated well by a quadratic function. The function $f$ is locally approximated by a Taylor series of degree two:
\begin{align}
f(x_n + p)&= f(x_n) + p^T \nabla_f(x_n) + \frac{1}{2} p^T H_f(x_n) p + \bigO(p^3) \nonumber\\
\frac{d}{dp}f(x_n) &= 0 \nonumber\\
\frac{d}{dp}f(x_n) &=\nabla_f(x_n) + H_f(x_n) p \nonumber\\
p &= H_f^{-1}(x_n) \nabla{f}(x_n) \nonumber\\
\text{taking~} p = x_{n+1} - x_n \nonumber
&\text{~we obtain~} x_{n+1} = x_n - H_f^{-1}(x_n) \nabla{f}(x_n)
\end{align}

Additionally, the iteration step is scaled by $\gamma \in ]0..1]$ to help with the convergence, leading to the following update:
\begin{align}
x_{n+1} = x_n - \gamma H_f^{-1}(x_n) \nabla{f}(x_n) \label{newtonstep}
\end{align}

In the case $H_f$ is not invertible, the hessian matrix is reconditioned such that $H_f~=~H_f~+~\lambda I$, with $\lambda$ a small value. Note that since we are inverting the Hessian matrix as in (\ref{newtonstep}), this algorithm is not suitable for functions with a high number of variables.

\subsection{Gradient Descent \nllref{OptimizerGradientDescent}}
The Newton's method is attempting to construct a sequence $x_n$ from an initial guess $x_0$ that converges towards $x_*$ such that the function $\nabla_f(x_n)=0$. Consequently finding a global extremum is not guaranteed. The closer the initial guess is from the global minimum, the more likely it is to find the global minimum.

It is assuming the function $f$ is differentiable. The parameters must be properly scaled (i.e., all the parameters are comparable and none of them is dominating the others). The update is simply:
\begin{align}
x_{n+1} = x_n - \lambda \nabla{f}(x_n) \label{gradientstep}
\end{align}
$\lambda$ must be sufficiently small to avoid the bouncing effect and sufficiently by for fast convergence. $\lambda$ is a single value if the parameters are scaled or can be a vector so that each parameter is scaled independently. The gradient can be normalized to have more comparable $\lambda$ accross problems.

If the parameters are not properly scaled (e.g., narrow valley), convergence may be extremely slow. In this case a second order method is advised.

\subsection{Hybrid Gradient Descent \nllref{OptimizerGradientDescentLineSearch}}
One problem with the Gradient Descent is that the step size $\lambda$ is constant (it might be a good step at the beginning of the optimization, but not so afterward) and needs to be chosen by hand with all the problems that implies. An alternative is to have a variable step size and its value being computed by a line search. Note that this approach requires more function evaluations. This leads to the following parameter update:

\begin{align}
& d_n = \frac{\nabla{f}(x_n)}{\lVert \nabla{f}(x_n) \lVert} \nonumber\\
& \lambda_n = lineSearch(x_n, d_n) \nonumber\\
& x_{n+1} = x_n - \lambda _n d_n
\end{align}

If the parameters are not properly scaled (e.g., narrow valley), convergence may be extremely slow. In this case a second order method is advised.

Very similarly, other optimizers are combined with a line search such as
\nllref{OptimizerNewtonLineSearch}

\subsection{Grid Search}
\subsection{Harmony Search}
\subsection{Genetic Algorithm}
\subsection{Line Search}
\subsection{Powell}
\subsection{Bracketing}

\section{Classifiers}
\subsection{Feed Forward Neural Networks}
\subsection{Radial Basis Function Neural networks}
\subsection{Gaussian Mixture Models \nllref{gmm}}
A gaussian mixture model (gmm) is a generative model modelling a distribution $p(X|\Theta)$ using a mixture of gaussian models. It is defined as following $p(x_i|\Theta)= \sum_{k=1}^K \lambda _k N(x_i|\mu _k, \Sigma _k)$ where $X=\{x_1,...,x_n\}$ is the training data and $\Theta$ represents the parameters of the gaussian functions $(\mu _k, \Sigma _k)$ and weights $\lambda _k$.

Assuming the samples are independent and identically distributed, we have the log-likelihood function defined as:
\begin{align*}
L(\Theta | X)&=\log \prod_{i=1}^N p(x_i|\Theta) \\
&=\sum_{i=1}^N \log p(x_i|\Theta) \\
&=\sum_{i=1}^N \sum_{k=1}^K \log \lambda _k N(x_i|\mu _k, \Sigma _k) \\
\end{align*}

We found that the parameters updates are:
\begin{align*}
\mu _ k &= \frac{1}{N_k} \sum_{i=1}^N \gamma _{ik} x_i \text{~with~} \\
\Sigma _k &= \frac{1}{N_k} \sum _{i=1}^N \gamma _{ik}(x_i-\mu _k)(x_i-\mu _k)^T \\
\lambda _k &= \frac{N_k}{N} \\
\end{align*}
With $N_k = \sum _{i=1}^N \gamma _{ik}$ and $\gamma _{ik} = \frac{\lambda_k N(x_i| \mu _k, \theta _k)}{\sum _{j=1}^K \lambda _j N(x_i| \mu _j, \theta _j)}$

~\\~
Intial values for $\lambda_k, \mu_k, \Sigma_k$ are found using K-means algorithm. For improved stability of the algorithm, $\Sigma_k$ is approximated by its diagonal.

\subsubsection{Derivation of the algorithm}
Recall that $\frac{d}{dx} \log f(x) = \frac{f'(x)}{f(x)}$ and $\frac{d}{dx} e ^x=x' e^x$, lets compute $\mu _k$:
\begin{align}
\frac{\partial L(\Theta | X)}{\partial\mu _k} &=  \sum_{i=1}^N \frac{\lambda_k N(x_i| \mu _k, \theta _k)}{\sum _{j=1}^K \lambda _j N(x_i| \mu _j, \theta _j)} \Sigma _k^{-1} (x_i-\mu _k) \nonumber\\
&=  \sum_{i=1}^N \gamma _{ik} \Sigma _k^{-1} (x_i-\mu _k) \text{~with ~} \nonumber\\
\text{rearranging~} \sum_{i=1}^N \gamma _{ik} \mu _ k &= \sum_{i=1}^N \gamma _{ik} x_i \nonumber\\
\bm{\mu _ k} &= \bm{\frac{1}{N_k} \sum_{i=1}^N \gamma _{ik} x_i} \text{~with~} N_k = \sum _{i=1}^N \gamma _{ik}
\end{align}

Recall that:
\begin{align*}
& \frac{d|M|}{dM} = |M|(M^-1)^T \\
& \frac{d(a^TMb)}{dM} = ab^T
\end{align*}

Lets name $c=e^{-\frac{1}{2}(x_n-\mu_k)\Sigma_k(x_n-\mu_k)}$. We have:
\begin{align*}
\frac{\partial c}{\partial \Sigma^{-1}_k} &= c(-\frac{1}{2})(x_n - \mu_k)(x_n - \mu_k)^T \\
\frac{\partial \lambda_k N(x_n|\mu_k,\Sigma_k)}{\partial\Sigma_k^{-1}} &= \frac{\partial(\frac{\lambda_k|\Sigma_k^{-1}|^{0.5}}{(2 \pi)^{D/2}}c)}{\partial \Sigma^{-1}_k} \\
&= \frac{\lambda_k c}{(2 \pi)^{D/2}}\frac{\partial(|\Sigma_k^{-1}|^{0.5})}{\Sigma_k^{-1}}
+ \frac{\lambda_k |\Sigma_k^{-1}|^{0.5}}{(2 \pi)^{D/2}} \frac{\partial c}{\partial \Sigma_k^{-1}} \\
&=\frac{\lambda_k c}{(2 \pi)^{D/2}}\frac{1}{2}|\Sigma_k^{-1}|^{-1/2} |\Sigma_k^{-1}| \Sigma_k
+ \frac{\lambda_k |\Sigma_k^{-1}|^{0.5}}{(2 \pi)^{D/2}}c{(-0.5(x_n-\mu_k)(x_n-\mu_k)^T)}\\
&= \frac{1}{2} \frac{\lambda_k |\Sigma_k^{-1}|^{0.5}c}{(2 \pi)^{D/2}}(\Sigma_k - (x_n-\mu_k)(x_n-\mu_k)^T)\\
&= \frac{1}{2} N(x_n|\mu_k, \Sigma_k)(\Sigma_k - (x_n-\mu_k)(x_n-\mu_k)^T) \\
\frac{\partial ln p(X|\Theta)}{\partial \Sigma_k^{-1}} &= \sum_{n=1}^N
\frac{1}{\sum_{j=1}^K \lambda_k N(x_n|\mu_j,\Sigma_j)}
\frac{\partial \lambda_k N(x_n|\mu_k,\Sigma_k)}{\partial\Sigma_k^{-1}} \\
&= \frac{1}{2} \sum_{n=1}^N \frac{\lambda_k N(x_n|\mu_k,\Sigma_k)(\Sigma_k-(x_n-\lambda_k)(x_n-\lambda_k)^T))}{\sum_{j=1}^K \lambda_j N(x_n|\mu_j,\Sigma_j)}\\
&=\frac{1}{2}\sum_{n=1}^N\gamma_{nk}(\Sigma_k-(x_n-\lambda_k)(x_n-\lambda_k)^T))
\end{align*}
Setting $\frac{\partial ln p(X|\Theta)}{\partial \Sigma_k^{-1}}$ to 0, we get:
\begin{align*}
\bm{\Sigma _k} &= \bm{\frac{1}{N_k} \sum _{n=1}^N \gamma _{nk}(x_n-\mu _k)(x_n-\mu _k)^T}
\end{align*}

Finally, compute $\lambda_k$
\begin{align*}
\frac{\partial ln p(X|\Theta)}{\partial \mu_k^{-1}} &= \sum_{n=1}^K \frac{N(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^K N(x_j|\mu_j,\Sigma_j)} \\
&= \sum_{n=1}^K \frac{\gamma_{nk}}{\lambda_k} \text{~(i.e., we just add the missing term from~} \gamma_{nk})
\end{align*}
We need to consider the constraint $\sum_{k=1}^K \lambda_k=1$ and form the Lagrangian

\begin{align*}
L &=\ln p(X|\Theta) + \lambda((\sum_{k=1}^K \lambda_k-1)) \\
\frac{\partial L}{\partial \lambda_k} &= 0 \\
0 &= \sum_{n=1}^K \frac{\gamma_{nk}}{\lambda_k} - \lambda \\
\sum_{k=1}^K \sum_{n=1}^N \gamma_{nk} &= \lambda \sum_{n=1}^K( \lambda_K) \\
\sum_{n=1}^N 1 &= \lambda \\
\lambda &= N\\
\text{Substituting we have:~} \bm{\lambda_k} &= \bm{\frac{N_k}{N}}
\end{align*}
\subsection{Boosting}
\subsection{Decision Tree}
\subsection{Stump}
\subsection{Perceptron}

\section{Data Preprocessing}
\subsection{Feature Selection}
blabla
\subsubsection{Locally Linear Embedding}
\subsubsection{Pearson Correlation Ratio}
\subsubsection{Relief-f}
\subsubsection{Wrapper Methods}

\subsection{Feature Reduction}
\subsubsection{Principal Component Analysis}
\subsubsection{Kernel Principal Component Analysis}
\subsubsection{Independent Component Analysis}
\subsubsection{Principal Component Analysis}
\subsection{Normalization}


\section{3D Visualization Algorithms}
\subsection{Interpolators}
\subsubsection{Nearest Neighbour}
\subsubsection{Trilinear}
\subsection{Transformation Models}
\subsubsection{Affine}
\subsubsection{Radial Basis functions}
\subsubsection{Dense Deformable Field}
\subsection{Multi-Planar Reconstruction}
\subsection{Maximum Intensity Projection}

\section{Notations}
function~$f$ is defined throughout the document as $f \colon \textbf{R}^n \to \textbf{R}$

\bibliography{nll-algorithm-documentation}
\bibliographystyle{plain}

\end{document}
